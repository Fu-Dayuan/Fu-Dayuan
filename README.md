## Hi there ğŸ‘‹

I am Dayuan Fu, a graduate student at [PRIS-NLP Group](https://pris-nlp.github.io/en/author/dayuan-fu/) at Beijing University of Posts and Telecommunications (BUPT), supervised by [Prof. Weiran Xu](https://pris-nlp.github.io/en/author/weiran-xu/). I'm also currently visiting [TsinghuaC3I group](https://c3i.ee.tsinghua.edu.cn/author/%E5%82%85%E5%A4%A7%E6%BA%90/). My research interests primarily focus on large language model agents, including both training-based and prompt-based approaches. For prompt-based approaches, I am concentrating on improving memory mechanisms. For training-based approaches, my focus is on agent generalization, such as creating a world model to handle diverse problems. Additionally, I am interested in the self-evolution of agents when tackling hard and large-scale questions, exemplified by projects like DeepSeek-Prover. I have published several papers at prominent NLP conferences, including EMNLP, CIKM, and NAACL.

Feel free to email me for any form of academic cooperation!

## ğŸ”¥ News

- 2024-09: ğŸ‰ğŸ‰ Two papers have been accepted by EMNLP 2024!
- 2024-03: ğŸ‰ğŸ‰ Two papers have been accepted by NAACL 2024!
- 2024-02: ğŸ‰ğŸ‰ One paper has been accepted by LREC-COLING 2024!
- 2023-08: ğŸ‰ğŸ‰ One paper has been accepted by CIKM 2023!
- 2023-02: ğŸ‰ğŸ‰ Two papers have been accepted by ICASSP 2023!
- 2022-10: ğŸ‰ğŸ‰ One paper has been accepted at the SereTOD 2022 Workshop, EMNLP 2022!
- 2022-09: ğŸ†ğŸ† Achieved the 1st rank on SereTOD 2022 track 2, EMNLP 2022!

## ğŸ“ Publications

1. MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making  
   **Dayuan Fu**, Biqing Qi, Yihuai Gao, Che Jiang, Guanting Dong, Bowen Zhou  
   EMNLP 2024 Main Conference [paper](https://arxiv.org/abs/2409.16686)
2. How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data  
   Yejie Wang\*, Keqing He\*, **Dayuan Fu\***, Zhuoma Gongque, Heyang Xu, Yanxu Chen, Zhexu Wang, Yujia Fu, Guanting Dong, Muxi Diao, Jingang Wang, Mengdi Zhang, Xunliang Cai, Weiran Xu  
   EMNLP 2024 Main Conference [paper](https://arxiv.org/abs/2409.03810)
3. AgentRefine: Enhancing Agent Generalization through Refinement Tuning  
   Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu  
   In process [link](https://openreview.net/forum?id=FDimWzmcWn)
4. PreAct: Prediction Enhances Agent's Planning Ability  
   **Dayuan Fu**, Jianzhao Huang, Siyuan Lu, Guanting Dong, Yejie Wang, Keqing He, Weiran Xu  
   In process [paper](https://arxiv.org/abs/arXiv:2402.11534)
5. On Large Language Models' Hallucination with Regard to Known Facts  
   Che Jiang, Biqing Qi, Xiangyu Hong, **Dayuan Fu**, Yang Cheng, Fandong Meng, Mo Yu, Bowen Zhou, Jie Zhou  
   NAACL 2024 Main Conference [paper](https://arxiv.org/abs/2403.20009)
6. DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations  
   Weihao Zeng\*, **Dayuan Fu\***, Keqing He, Yejie Wang, Yukai Xu, Weiran Xu  
   NAACL 2024 Findings [paper](https://arxiv.org/abs/2404.00557)
7. Semi-supervised knowledge-grounded pre-training for task-oriented dialog systems  
   Weihao Zeng, Keqing He, Zechen Wang, **Dayuan Fu**, Guanting Dong, Ruotong Geng, Pei Wang, Jingang Wang, Chaobo Sun, Wei Wu, Weiran Xu  
   SereTOD 2022 Workshop, EMNLP 2022, the 1st Award on SereTOD Challenge 2022 track 2 [paper](https://arxiv.org/abs/2210.08873)
8. CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery  
   Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, **Dayuan Fu**, Huangxuan Wu, Bin Liang, Weihao Zeng, Yejie Wang, Zhuoma GongQue, Jianing Yu, Qiuna Tan, Weiran Xu  
   In process [paper](https://arxiv.org/abs/arXiv:2406.08587)

## ğŸ– Competitions and Awards
- National Scholarship in China (2021)
- 2022-09: ğŸ†ğŸ† Achieved the 1st Award on SereTOD Challenge 2022 track 2, EMNLP 2022!


<!--
**Fu-Dayuan/Fu-Dayuan** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ğŸ”­ Iâ€™m currently working on ...
- ğŸŒ± Iâ€™m currently learning ...
- ğŸ‘¯ Iâ€™m looking to collaborate on ...
- ğŸ¤” Iâ€™m looking for help with ...
- ğŸ’¬ Ask me about ...
- ğŸ“« How to reach me: ...
- ğŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
